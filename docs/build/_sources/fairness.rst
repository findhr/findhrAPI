Fairness Intervention for Fair Rankings
=======================================
The European General Data Protection Regulation (GDPR) prohibits the processing of certain categories of sensitive information, except for age and gender. Even if one has access to the sensitive information of the candidates, it is possible that not all candidates feel comfortable disclosing this information with the platform. Having limited access to the sensitive information of the candidates makes the use of existing fairness interventions hard to use in practice.

Considering the limited access to sensitive information pre-processing fairness interventions are more suitable to be used in practice when trying to ensure the fair ranking of candidates within a recruitment system. Pre-processing methods can be trained offline on a training set containing sensitive information, which was acquired in compliance with the GDPR, while during inference time, the pre-processing method can be applied without needing access to sensitive information.

The Software Module developed as part of this deliverable supports three state of the art pre-processing fairness intervention methods. Below is a brief description of each method:

(1) LFR [3]: aims at achieving both group and individual fairness by creating representations that obfuscate information about the protected groups, while also ensuring a good encoding of useful information (which aims at preserving the utility of the ranking).

(2) iFair [2]: aims at achieving individual fairness by creating representations that encourage similar outcomes for similar individuals regardless of the sensitive attributes, while also ensuring a good encoding of useful information (which aims at preserving the utility of the ranking).

(3) gFair: an adaptation of iFair. aims at achieving group fairness by creating representations that encourage similar outcomes between similar individuals of different groups, regardless of the sensitive attributes. Moreover, it aims at preserving the in-group fairness as well as preserving the usefulness of the representations for the ranking task.



.. csv-table:: 
   :header: "Name", "Fairness Notion", "Compound Loss", "Hyperparameters", "Hyperparameter Effects"
   :widths: 10, 20, 25, 35, 45

   "LFR", "Group Fairness", "L = Ax * Lx + Ay * Ly + Az * Lz", "Lx: reconstruction loss; Ly: utility loss; Lz: group fairness loss; Ax, Ay, Az: weights", "Increase Ax to preserve original structure (may reduce fairness); Increase Ay to improve utility; Increase Az to enhance group fairness constraints"
   "iFair", "Individual Fairness", "L = Ax * Lx + Az * Lz", "Lx: reconstruction loss; Lz: individual fairness loss; Ax, Az: weights", "Increase Ax to preserve original structure; Increase Az to enhance individual fairness constraints"
   "gFair", "Group Fairness", "L = Ax * Lx + Aigf * Ligf + Az * Lz", "Lx: reconstruction loss; Ligf: in-group fairness loss; Lz: group fairness loss; Ax, Aigf, Az: weights", "Increase Ax to preserve original structure; Increase Aigf for in-group fairness; Increase Az to enhance group fairness"


All methods support non-binary groups.

LFR and gFair support different type of optimisation for achieving group fairness for multiple groups or intersectional groups:

.. csv-table:: 
   :header: "Optimization Type", "Comparison Strategy", "Description"
   :widths: 20, 25, 55

   "Independent", "Per sensitive attribute", "Each sensitive attribute is optimized independently, without considering interactions between them."
   "Pairwise", "All intersectional pairs", "Considers all unique comparisons between intersectional sub-groups during optimization. Captures interactions but is computationally expensive."
   "Dynamic", "Max-gap optimisation", "In each iteration, computes fairness loss based only on the sub-group pair with the largest disparity. Reduces optimization complexity, though all comparisons are still evaluated to identify the max gap."
   "Control (A or D)", "All vs control group", "Compares all sub-groups to a single control group. Control A: reduces disparities with the advantaged group but may reinforce it as the norm. Control D: highlights disparities relative to the disadvantaged group."
   "Extremes", "Most advantaged vs least", "Only compares the most privileged and the most disadvantaged sub-groups. Simplifies the problem and highlights the largest disparity, potentially benefiting intermediate groups."


Using the pre-processing fairness interventions to generate a fair ranking:

(1) Option 1: Re-rank the candidates based on the fair score generated by the Fairness Intervention Model.
(2) Option 2: Use the fair data representation of the candidates to train a Ranking Model. The pre-processing fairness interventions can be applied on the data representing the candidates to create fairer representations independent of the ranking model.

Run Example
-----------

Running example can be found in the notebook provided Example_FairPreprocessingRanking.ipynb. The output of running the notebook can be found under ./data/fairness_output.
If you don't want to use the scikit pipeline tu run the fairness pre-processing interventions you can do the following:

.. code-block:: python

   fair_model.fit(df_all_train)
   df_all_fair_test = fair_model.transform(df_all_test)

References
----------
[1]
Rus, Clara, Maarten de Rijke, and Andrew Yates. "A Study of Pre-processing Fairness Intervention
Methods for Ranking People." (2024).

[2]
Preethi Lahoti, Krishna P Gummadi, and Gerhard Weikum. 2019. ifair: Learning individually fair data representations for algorithmic decision
making. In 2019 IEEE 35th International Conference on Data Engineering (ICDE). IEEE, 1334â€“1345.

[3]
Zemel, Rich, et al. "Learning fair representations." International conference on machine learning. PMLR, 2013.
