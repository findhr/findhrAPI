{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad2239af",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Explainable Boosting Machine (EBM) Model\n",
    "This notebook shows an example of how to use the APIs with the ExplainableBoostingMachine model from the [interpret](https://interpret.ml/docs/ebm.html) package and how to get a factual explanation for the model in the form of feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e11db1",
   "metadata": {
    "scrolled": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Load and join raw data sources and their metadata.\n",
    "%run Example_InputDataSources.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774199b5",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Joined DataFrame.\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f69d5",
   "metadata": {
    "scrolled": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Joined metadata.\n",
    "md_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Importing libraries to avoid warnings at running time\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(transform_output = \"pandas\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb9ceb",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Setting category columns in DataFrame based on metadata.\n",
    "cat_cols = [k for k, v in md_all.items() if v.attr_type=='category']\n",
    "df_all[cat_cols] = df_all[cat_cols].astype('category')\n",
    "# Dataframe metadata.\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc65dcb",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Define ids, target feature(s), and predictive features.\n",
    "id_cols = ['id_j', 'id_c']\n",
    "target_cols = ['score', 'ranking', 'shortlisted']\n",
    "pred_cols = df_all.columns.difference(target_cols + id_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from findhr.preprocess.example_mappings import RelevantExperienceForRole, ExtractMonthDurationJob, MatchOrdinal, \\\n",
    "    ExtractListOfProperty, MatchFeatureAtLeastInList, MatchFeatureSet, MatchBinary\n",
    "\n",
    "# Calculated features.\n",
    "maps_derived_1 = {\n",
    "    (('professional_experience_c', 'experience_reqs_role_j',), ('relevant_exp_role_c',)): RelevantExperienceForRole(),\n",
    "}\n",
    "\n",
    "maps_derived_2 = {\n",
    "    (('relevant_exp_role_c',), ('role_duration_months_c',)): ExtractMonthDurationJob(duration_key='duration_months'),\n",
    "    (('education_background_c',), ('degree_list_c',)): ExtractListOfProperty(property_key='degree')\n",
    "}\n",
    "\n",
    "# Fitness features about the matching between candidate's features and job's requirements.\n",
    "maps_matching = {\n",
    "    (('experience_reqs_duration_j', 'role_duration_months_c'), ('fitness_experience',)): MatchOrdinal(),\n",
    "    (('education_reqs_j', 'education_background_c'), ('fitness_education',)): MatchFeatureAtLeastInList(),\n",
    "    (('skills_j', 'skills_c'), ('fitness_skills',)): MatchFeatureSet(),\n",
    "    (('gender_j', 'gender_c'), ('fitness_gender',)): MatchBinary(),\n",
    "    (('agg_perceived_foreign_j', 'agg_perceived_foreign_c'), ('fitness_foreign',)): MatchBinary()\n",
    "}\n",
    "\n",
    "# Helper variable for the fitness features\n",
    "list_cols_fitness = ['fitness_experience', 'fitness_education', 'fitness_skills', 'fitness_gender', 'fitness_foreign']\n",
    "maps_matching"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scikit-learn transformation for numeric and categorical features\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numeric_features = list_cols_fitness\n",
    "categorical_features = ['gender_c', 'agg_perceived_foreign_c']\n",
    "# imputing and scaling numeric features\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),  # Not needed for the used dataset.\n",
    "        (\"scaler\", StandardScaler())  # Not needed for the decision tree, let's keep it for the sake of generality.\n",
    "    ]\n",
    ")\n",
    "# imputing and encoding categorical features\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        # Not needed for the used dataset, again for the sake of generality.\n",
    "        (\"encoder\", OneHotEncoder()),  # Convert to one-hot encoding\n",
    "    ]\n",
    ")\n",
    "# combining the two above\n",
    "column_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        # (\"cat\", categorical_transformer, categorical_features)\n",
    "    ],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from findhr.preprocess.mapping import AttachMetadata, DetachMetadata, DerivedColumn\n",
    "\n",
    "# The pipeline is composed of two phases:\n",
    "# 1. Preprocessing with metadata (using findhr package)\n",
    "pipeline_derived = Pipeline(steps=[\n",
    "    (\"init\", AttachMetadata(md_all)),\n",
    "    ('mapping_1', DerivedColumn(maps_derived_1)),\n",
    "    ('mapping_2', DerivedColumn(maps_derived_2)),\n",
    "    (\"matching\", DerivedColumn(maps_matching)),\n",
    "    # (\"fitness\", GroundTruthLinearWeightedScorer(gt_weights_fair)),\n",
    "    (\"end\", DetachMetadata())\n",
    "])\n",
    "# 2. Standard scikit-learn preprocessing to prepare the data for the model covered by column preprocessor.\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Pipeline Including ExplainableBoostingRegressor"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d75a1b",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Pipeline definition for regression model on the target feature \"score\".\n",
    "from findhr.preprocess.mapping import AttachMetadata, DerivedColumn, DetachMetadata\n",
    "from interpret.glassbox import ExplainableBoostingRegressor\n",
    "pipeline_regr = Pipeline(\n",
    "    steps=[\n",
    "        # first phase: preprocessing with metadata\n",
    "        ('fitness_value', pipeline_derived\n",
    "         ),\n",
    "        # second phase: preprocessing without metadata (standard scikit-learn)\n",
    "        (\"column_preprocessor\", column_preprocessor),\n",
    "        # model inference\n",
    "        (\"regressor\", ExplainableBoostingRegressor())\n",
    "       ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ccffae",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Model fit.\n",
    "pipeline_regr.fit(df_all.loc[:, pred_cols], df_all.loc[:, 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7c52b",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Example model prediction.\n",
    "pipeline_regr.predict(df_all.loc[:10, pred_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example Model Explanation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Get a global explanation from ExplainableBoostingRegressor\n",
    "explanation_global = pipeline_regr.named_steps['regressor'].explain_global()#name=list_cols_fitness)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the global explanation through plotting the feature importance.\n",
    "explanation_global.visualize()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the transformed data at the end before the model prediction.\n",
    "idx_explicand_sample = 0\n",
    "explicand_sample = df_all.loc[:, pred_cols].iloc[idx_explicand_sample:idx_explicand_sample+1]\n",
    "transformed_data = pipeline_regr[:-1].transform(explicand_sample)\n",
    "explanation_local = pipeline_regr.named_steps['regressor'].explain_local(transformed_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the local explanation for the first sample explained.\n",
    "# See documentation at https://interpret.ml/docs/ebm.html for further details.\n",
    "explanation_local.visualize(0)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
