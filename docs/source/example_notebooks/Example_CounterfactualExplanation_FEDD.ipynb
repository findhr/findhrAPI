{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Counterfactual Explanation for the Feature Engineering Data-Domain (FEDD)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from collections import namedtuple\n",
    "from lightgbm import LGBMRanker\n",
    "from findhr.preprocess.mapping import AttachMetadata, DetachMetadata, DerivedColumn\n",
    "from findhr.preprocess.example_mappings import RelevantExperienceForRole, ExtractMonthDurationJob, MatchOrdinal, ExtractListOfProperty, MatchFeatureAtLeastInList, MatchFeatureSet, MatchBinary\n",
    "\n",
    "from findhr.xai.counterfactual import dice_ml\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Helper Classes and Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%run ./Example_InputDataSources.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Joined DataFrame.\n",
    "df_all.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Joined metadata.\n",
    "md_all"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define helper function\n",
    "def rank2relevance(df, top_k, col_rank):\n",
    "    return top_k + 1 - df[col_rank].values.ravel()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the Preprocessing pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define subsets of columns\n",
    "cols_id = ['id_c', 'id_j']\n",
    "all_cols = ['id_c', 'education_background_c', 'professional_experience_c',\n",
    "   'skills_c', 'gender_c', 'agg_perceived_foreign_c', 'id_j',\n",
    "   'education_reqs_j', 'experience_reqs_role_j',\n",
    "   'experience_reqs_duration_j', 'skills_j', 'gender_j',\n",
    "   'agg_perceived_foreign_j', 'ranking', 'shortlisted', 'score']\n",
    "# Define the subset of columns of the HUDD dataset describing the candidate,\n",
    "# which are used in the preprocessing+prediction pipeline\n",
    "cols_c = ['education_background_c', 'professional_experience_c',\n",
    "          'skills_c', 'gender_c', 'agg_perceived_foreign_c']\n",
    "cols_j = ['education_reqs_j', 'experience_reqs_role_j',\n",
    "   'experience_reqs_duration_j', 'skills_j', 'gender_j', 'agg_perceived_foreign_j']\n",
    "cols_pred_preprocess = cols_c + cols_j\n",
    "cols_not_for_pred = ['id_c', 'id_j', 'ranking', 'shortlisted']\n",
    "cols_sensitive = ['gender_c', 'agg_perceived_foreign_c']\n",
    "col_target = ['score']\n",
    "\n",
    "df_CDS_JDS = df_all[cols_id + [col for col in df_all if col not in cols_id+col_target] + col_target ]\n",
    "\n",
    "cols_dict_HUDD = {'cols_pred_preprocess': cols_pred_preprocess,\n",
    "                    'cols_sensitive': cols_sensitive,\n",
    "                    'cols_id': cols_id,\n",
    "                    'cols_not_for_pred': cols_not_for_pred,\n",
    "                    'col_target': col_target}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_fitness_matrix(df_CDS_JDS, cols_dict):\n",
    "    # Calculated features.\n",
    "    maps_derived_1 = {\n",
    "        (('professional_experience_c', 'experience_reqs_role_j',), ('relevant_exp_role_c',)): RelevantExperienceForRole(),\n",
    "    }\n",
    "\n",
    "    maps_derived_2 = {\n",
    "            (('relevant_exp_role_c',), ('role_duration_months_c',)): ExtractMonthDurationJob(duration_key='duration_months'),\n",
    "            (('education_background_c',), ('degree_list_c',)): ExtractListOfProperty(property_key='degree')\n",
    "    }\n",
    "\n",
    "    # Fitness features about the matching between candidate's features and job's requirements.\n",
    "    maps_matching = {\n",
    "        (('experience_reqs_duration_j', 'role_duration_months_c'), ('fitness_experience',)): MatchOrdinal(),\n",
    "        (('education_reqs_j', 'education_background_c'), ('fitness_education',)): MatchFeatureAtLeastInList(),\n",
    "        (('skills_j', 'skills_c'), ('fitness_skills',)): MatchFeatureSet(),\n",
    "        (('gender_j', 'gender_c'), ('fitness_gender',)): MatchBinary(),\n",
    "        (('agg_perceived_foreign_j', 'agg_perceived_foreign_c'), ('fitness_foreign',)): MatchBinary()\n",
    "    }\n",
    "\n",
    "\n",
    "    # Calculation as fit-transform preprocessing\n",
    "    pipeline_fitness = Pipeline(steps=[\n",
    "        (\"init\", AttachMetadata(md_all)),\n",
    "        ('mapping_1', DerivedColumn(maps_derived_1)),\n",
    "        ('mapping_2', DerivedColumn(maps_derived_2)),\n",
    "        (\"matching\", DerivedColumn(maps_matching)),\n",
    "        # (\"fitness\", GroundTruthLinearWeightedScorer(gt_weights_fair)),\n",
    "        (\"end\", DetachMetadata())\n",
    "    ])\n",
    "\n",
    "    pipeline_fitness.fit(X=df_CDS_JDS)\n",
    "    fitness_matrix = pipeline_fitness.transform(X=df_CDS_JDS)\n",
    "    df_fitness_mat = fitness_matrix.copy(deep=True)\n",
    "    columns_keep = cols_dict['cols_id'] + \\\n",
    "                   [col for col in fitness_matrix if\n",
    "                    col.startswith('fitness_')] + cols_dict['cols_sensitive'] + cols_dict['col_target']\n",
    "\n",
    "    df_fitness_mat = df_fitness_mat[columns_keep]\n",
    "\n",
    "    # From scores, we can learn regressors; or we can produce ranks, and learn ranking models\n",
    "    df_fitness_mat['rank'] = df_fitness_mat.groupby(\"id_j\")['score'].rank('dense', ascending=False)\n",
    "    df_fitness_mat['rank'] = df_fitness_mat['rank'].apply(lambda x: x if x <= TOP_K else TOP_K + 1)\n",
    "\n",
    "    return pipeline_fitness, df_fitness_mat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the ranking model\n",
    "To better understand the LambdaRank framework, we point to the relevant literature (e.g. [Burges, 2010, From RankNet to LambdaRank to LambdaMART: An Overview](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def data_split_FEDD(df_fitness_mat):\n",
    "    all_jobs = df_fitness_mat['id_j'].unique()\n",
    "    # For this example, we assume that train, val and test sets are the same\n",
    "    train_jobs = test_jobs = val_jobs = all_jobs\n",
    "    # train_jobs, test_jobs = train_test_split(all_jobs, test_size=0.5, random_state=42, # shuffle=False)\n",
    "    # train_jobs, val_jobs = train_test_split(train_jobs, test_size=0.25, random_state=42, shuffle=False)\n",
    "\n",
    "    # Build train, test and validation sets, ensuring they are sorted by id_j, id_c\n",
    "    df_train = df_fitness_mat[df_fitness_mat['id_j'].isin(train_jobs)].sort_values([\"id_j\", \"id_c\"])\n",
    "    df_val = df_fitness_mat[df_fitness_mat['id_j'].isin(val_jobs)].sort_values([\"id_j\", \"id_c\"])\n",
    "    df_test = df_fitness_mat[df_fitness_mat['id_j'].isin(test_jobs)].sort_values([\"id_j\", \"id_c\"])\n",
    "\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "\n",
    "def init(df_fitness_mat):\n",
    "    df_train, df_val, df_test = data_split_FEDD(df_fitness_mat)\n",
    "    # Define subsets of columns\n",
    "    cols_id = ['id_j', 'id_c']  # ids\n",
    "    cols_pred = sorted([  # predictive\n",
    "        'fitness_experience',\n",
    "        'fitness_education',\n",
    "        'fitness_skills',\n",
    "        'fitness_gender',\n",
    "        'fitness_foreign'])\n",
    "    cols_sensitive = ['gender_c']  # sensitive attribute(s)\n",
    "    col_target = 'score'  # target value for ranking\n",
    "    col_rank = 'rank'  # rank value for ranking\n",
    "\n",
    "    cols_dict_FEDD = {'cols_id': cols_id,\n",
    "                      'cols_pred': cols_pred,\n",
    "                      'cols_sensitive': cols_sensitive,\n",
    "                      'col_target': col_target,\n",
    "                      'col_rank': col_rank}\n",
    "    # Define the ranking model\n",
    "    ranker = LGBMRanker(\n",
    "        objective=\"lambdarank\",\n",
    "        class_weight=\"balanced\",\n",
    "        boosting_type=\"gbdt\",\n",
    "        importance_type=\"gain\",\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=10,\n",
    "        force_row_wise=True,\n",
    "        n_jobs=-1,  # max parallelism\n",
    "        verbose=-1  # no verbosity\n",
    "    )\n",
    "    return ranker, df_train, df_val, df_test, cols_dict_FEDD\n",
    "\n",
    "\n",
    "def train(ranker, df_train, df_val, cols_dict):\n",
    "    df_train_counts = df_train.groupby(\"id_j\")[\"id_j\"].count().to_numpy()\n",
    "    df_val_counts = df_val.groupby(\"id_j\")[\"id_j\"].count().to_numpy()\n",
    "\n",
    "    # Fitting ranker:\n",
    "    ranker.fit(\n",
    "        X=df_train[cols_dict['cols_pred']],\n",
    "        # LightGBM relevance is the higher the better\n",
    "        y=rank2relevance(df_train, TOP_K, cols_dict['col_rank']),\n",
    "        group = df_train_counts,\n",
    "        eval_at = [TOP_K],\n",
    "        # LightGBM relevance is the higher the better\n",
    "        eval_set =[(df_val[cols_dict['cols_pred']], rank2relevance(df_val, TOP_K, cols_dict['col_rank']))],\n",
    "        eval_group =[df_val_counts]\n",
    "    )\n",
    "\n",
    "    return ranker\n",
    "\n",
    "\n",
    "def evaluate(ranker, df_test, cols_dict):\n",
    "    df_test_counts = df_test.groupby(\"id_j\")[\"id_j\"].count().to_numpy()\n",
    "    # Predicting ranker:\n",
    "    df_test['lambda'] = ranker.predict(df_test[cols_dict['cols_pred']])\n",
    "    df_test['pred_rank'] = df_test.groupby(\"id_j\")['lambda'].rank('dense', ascending=False)\n",
    "    df_test['pred_rank'] = df_test['pred_rank'].apply(lambda x: x if x <= TOP_K else TOP_K + 1)\n",
    "\n",
    "    return df_test\n",
    "\n",
    "\n",
    "def ranking_pipeline(df_fitness_mat):\n",
    "    ranker, df_train, df_val, df_test, cols_dict_FEDD = init(df_fitness_mat)\n",
    "    ranker = train(ranker, df_train, df_val, cols_dict_FEDD)\n",
    "    df_test = evaluate(ranker, df_test, cols_dict_FEDD)\n",
    "    return ranker, df_test, cols_dict_FEDD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare data and functions for the counterfactual explanation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def define_cols_dict():\n",
    "    outcome_name_col = 'lambda'  # 'pred_rank'\n",
    "    continuous_features = sorted(['fitness_skills', 'fitness_gender','fitness_foreign'])\n",
    "    categorical_features = sorted(['fitness_education', 'fitness_experience'])\n",
    "    cols_pred = sorted(continuous_features + categorical_features)\n",
    "    return {'outcome_name_col': outcome_name_col, 'continuous_features': continuous_features,\n",
    "            'categorical_features': categorical_features, 'cols_pred': cols_pred}\n",
    "\n",
    "\n",
    "def extract_explicand_data_cf(id_j, candidate_position, df_test, ranker, cols_dict_FEDD, df_CDS_JDS):\n",
    "    # Extract the data of the candidate to be explained\n",
    "\n",
    "    # df_id_j contains the data for the job id_j\n",
    "    df_id_j_FEDD = df_test[df_test['id_j'] == id_j]\n",
    "    # Add the lambda and the predicted rank for all the candidates\n",
    "    df_id_j_FEDD['lambda'] = ranker.predict(df_id_j_FEDD[cols_dict_FEDD['cols_pred']])\n",
    "    df_id_j_FEDD['pred_rank'] = df_id_j_FEDD.groupby(\"id_j\")['lambda'].rank('dense', ascending=False)\n",
    "\n",
    "    exp_c_pred_rank = candidate_position\n",
    "\n",
    "    # Extract the explicand candidate id_c\n",
    "    exp_c_id_c = df_id_j_FEDD.loc[df_id_j_FEDD['pred_rank'] == exp_c_pred_rank, 'id_c'].iloc[0]\n",
    "\n",
    "    # Isolate the candidates' profiles applying for the job id_j\n",
    "    df_id_j_HUDD = df_CDS_JDS[df_CDS_JDS['id_j'] == id_j]\n",
    "\n",
    "    # Isolate the explicand candidate profile\n",
    "    exp_c_profile = df_CDS_JDS[df_CDS_JDS['id_c'] == exp_c_id_c]\n",
    "\n",
    "    exp_c = {'id_c': exp_c_id_c, 'profile': exp_c_profile}\n",
    "\n",
    "    cols_dict = define_cols_dict()\n",
    "\n",
    "    return df_id_j_FEDD, df_id_j_HUDD, exp_c, cols_dict\n",
    "\n",
    "\n",
    "def prepare_data_cf(df_id_j_FEDD, cols_dict):\n",
    "\n",
    "    # Convert data types\n",
    "    df_id_j_FEDD_pre = df_id_j_FEDD[cols_dict['categorical_features']].astype('int').copy(deep=True)\n",
    "    df_id_j_FEDD_pre[cols_dict['continuous_features']] = df_id_j_FEDD[cols_dict['continuous_features']].astype('float').copy(deep=True)\n",
    "    df_id_j_FEDD_pre[cols_dict['outcome_name_col']] = df_id_j_FEDD[cols_dict['outcome_name_col']].copy(deep=True)\n",
    "    feature_dtypes = {col: df_id_j_FEDD_pre[col].dtype for col in df_id_j_FEDD_pre[cols_dict['cols_pred']].columns}\n",
    "\n",
    "    return df_id_j_FEDD_pre, feature_dtypes\n",
    "\n",
    "\n",
    "def define_target(args, df_id_j_FEDD, target_rank=TOP_K):\n",
    "    # 'in_top_k' or 'out_top_k' depending on the candidate position\n",
    "    explicand_class = 'in_top_k' if args.candidate_position <= target_rank else 'out_top_k'\n",
    "\n",
    "    # target rank for counterfactual explanation\n",
    "    if args.target_rank:\n",
    "        tgt_cf_rank = args.target_rank\n",
    "        tgt_cf_score = df_id_j_FEDD[df_id_j_FEDD['pred_rank'] == tgt_cf_rank]['score'].iloc[0]\n",
    "        tgt_cf_candidate = df_id_j_FEDD[df_id_j_FEDD['pred_rank'] == tgt_cf_rank]\n",
    "\n",
    "    elif args.target_score:\n",
    "        tgt_cf_rank = None\n",
    "        tgt_cf_score = args.target_score\n",
    "        tgt_cf_candidate = None\n",
    "    else:\n",
    "        raise ValueError('Either target rank or target score must be provided')\n",
    "\n",
    "    return explicand_class, tgt_cf_rank, tgt_cf_score, tgt_cf_candidate\n",
    "\n",
    "\n",
    "def define_explainer_FEDD(ranker, df_id_j_FEDD_pre, cols_dict_cf, feature_dtypes, explanation_method, target_rank=TOP_K):\n",
    "    data_dice = dice_ml.Data(dataframe=df_id_j_FEDD_pre[cols_dict_cf['cols_pred'] + [cols_dict_cf['outcome_name_col']]],\n",
    "                             continuous_features=cols_dict_cf['continuous_features'],\n",
    "                             categorical_features=cols_dict_cf['categorical_features'],\n",
    "                             outcome_name=cols_dict_cf['outcome_name_col'])\n",
    "\n",
    "    kwargs = {'top_k': target_rank, 'features_dtype': feature_dtypes}\n",
    "\n",
    "    model_dice = dice_ml.Model(model=ranker,\n",
    "                               backend={'explainer': 'dice_xgboost.DiceGenetic',\n",
    "                                        'model': \"lgbmranker_model.LGBMRankerModel\"},\n",
    "                               model_type=\"regressor\",\n",
    "                               # model_type=\"classifier\",\n",
    "                               kw_args=kwargs)\n",
    "\n",
    "    explainer = dice_ml.Dice(data_dice, model_dice, method=explanation_method)\n",
    "\n",
    "    return explainer, data_dice, model_dice\n",
    "\n",
    "\n",
    "def get_explanations_FEDD(df_id_j_FEDD, exp_c, cols_dict_cf, explainer, target_rank=TOP_K):\n",
    "\n",
    "    # Get the predicted lambda for the candidate in target_rank position\n",
    "    c_th_lambda = df_id_j_FEDD[df_id_j_FEDD['pred_rank'] == target_rank].iloc[0]['lambda']\n",
    "    print(f'Predicted lambda for candidate in rank {target_rank}: {c_th_lambda}')\n",
    "    # Generate the counterfactual explanations\n",
    "    explanations = explainer.generate_counterfactuals(exp_c['profile'][cols_dict_cf['cols_pred']],\n",
    "                                                      total_CFs=4,\n",
    "                                                      desired_range=[c_th_lambda, 100], # Look for alternative candidates with a higher lambda than c_th_lambda\n",
    "                                                      # desired_class=\"opposite\",\n",
    "                                                      verbose=True)\n",
    "    return explanations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute counterfactual explanation for an instance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The job id for which the counterfactual explanation is to be generated'\n",
    "# Valid values 1-5\n",
    "id_j = 1\n",
    "\n",
    "# The position of the candidate to explain the prediction\n",
    "candidate_position = 12\n",
    "\n",
    "# Alternative ways to define the target of the explanation (less than or equal to TOP_K):\n",
    "target_rank = TOP_K\n",
    "\n",
    "# Select the algorithm for generating the counterfactual explanation:\n",
    "# valid values are 'genetic' or 'random'\n",
    "explanation_method = 'genetic'\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Helper structure to save the target rank and candidate position for the explanations\n",
    "args = namedtuple('Args', ['target_rank', 'target_candidate'])\n",
    "\n",
    "args.target_rank = target_rank\n",
    "args.candidate_position = candidate_position"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the pipeline:\n",
    "# - first the fitness pipeline\n",
    "# - then the ranking pipeline\n",
    "pipeline_fitness, df_fitness_mat = build_fitness_matrix(df_CDS_JDS, cols_dict_HUDD)\n",
    "ranker, df_test, cols_dict_FEDD = ranking_pipeline(df_fitness_mat)\n",
    "\n",
    "# Extract the data of the candidate to be explained\n",
    "df_id_j_FEDD, df_id_j_HUDD, exp_c, cols_dict_cf = extract_explicand_data_cf(id_j, candidate_position, df_test, ranker, cols_dict_FEDD, df_CDS_JDS)\n",
    "\n",
    "# Prepare the data for the explanations. Define the column data types\n",
    "df_id_j_FEDD_pre, feature_dtypes = prepare_data_cf(df_id_j_FEDD, cols_dict_cf)\n",
    "\n",
    "# Extract the target for the counterfactual explanation (i.e., the data of the candidate in the target position)\n",
    "explicand_class, tgt_cf_rank, tgt_cf_score, tgt_cf_candidate = define_target(args, df_id_j_FEDD, target_rank=target_rank)\n",
    "\n",
    "# Define the counterfactual explainer for the ranker\n",
    "explainer, data_dice, model_dice = define_explainer_FEDD(ranker, df_id_j_FEDD_pre, cols_dict_cf, feature_dtypes, explanation_method, target_rank=target_rank)\n",
    "\n",
    "# Generate the counterfactual explanations\n",
    "explanations_FEDD = get_explanations_FEDD(df_id_j_FEDD, exp_c, cols_dict_cf, explainer, target_rank=target_rank)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compare original instance with the obtained counterfactuals"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(explanations_FEDD.cf_examples_list.visualize_as_dataframe(show_only_changes=True))\n",
    "print(explanations_FEDD.visualize_as_dataframe(show_only_changes=True))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The query instance has a original predicted outcome of about -0.11. The obtained counterfactual have a greater lambda of 0.13.\n",
    "The counterfactual explanations shows that the candidate should improve the skills (from fitness_skill=0.2 to 1.0), to reach the selected top 10 candidates."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
