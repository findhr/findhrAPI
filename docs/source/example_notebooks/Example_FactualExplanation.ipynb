{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad2239af",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Factual Explanation of LambdaMART\n",
    "This notebook shows an example of how to use the APIs with the ExplainableBoostingMachine model from the [interpret](https://interpret.ml/docs/ebm.html) package and how to get a factual explanation for the model in the form of feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e11db1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load and join raw data sources and their metadata.\n",
    "%run Example_InputDataSources.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774199b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joined DataFrame.\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f69d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Joined metadata.\n",
    "md_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from findhr.preprocess.example_mappings import RelevantExperienceForRole, ExtractMonthDurationJob, MatchOrdinal, ExtractListOfProperty, MatchFeatureAtLeastInList, MatchFeatureSet, MatchBinary\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from findhr.preprocess.mapping import AttachMetadata, DetachMetadata, DerivedColumn\n",
    "from lightgbm import LGBMRanker\n",
    "from sklearn.metrics import ndcg_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Importing libraries to avoid warnings at running time\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb9ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting category columns in DataFrame based on metadata.\n",
    "cat_cols = [k for k, v in md_all.items() if v.attr_type=='category']\n",
    "df_all[cat_cols] = df_all[cat_cols].astype('category')\n",
    "# Dataframe metadata.\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc65dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ids, target feature(s), and predictive features.\n",
    "id_cols = ['id_c', 'id_c']\n",
    "target_cols = ['score', 'ranking', 'shortlisted']\n",
    "pred_cols = df_all.columns.difference(target_cols + id_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For this example we assume that the training, validation and test set coincides\n",
    "df_train = df_val = df_test = df_all\n",
    "\n",
    "df_train_counts = df_all.groupby(\"id_j\")[\"id_j\"].count().to_numpy()\n",
    "df_val_counts = df_all.groupby(\"id_j\")[\"id_j\"].count().to_numpy()\n",
    "df_train_counts, df_val_counts\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Build the preprocessing and prediction pipelines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372fcc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated features.\n",
    "maps_derived_1 = {\n",
    "    (('professional_experience_c', 'experience_reqs_role_j',), ('relevant_exp_role_c',)): RelevantExperienceForRole(),\n",
    "}\n",
    "\n",
    "maps_derived_2 = {\n",
    "        (('relevant_exp_role_c',), ('role_duration_months_c',)): ExtractMonthDurationJob(duration_key='duration_months'),\n",
    "        (('education_background_c',), ('degree_list_c',)): ExtractListOfProperty(property_key='degree')\n",
    "}\n",
    "\n",
    "# Fitness features about the matching between candidate's features and job's requirements.\n",
    "maps_matching = {\n",
    "    (('experience_reqs_duration_j', 'role_duration_months_c'), ('fitness_experience',)): MatchOrdinal(),\n",
    "    (('education_reqs_j', 'education_background_c'), ('fitness_education',)): MatchFeatureAtLeastInList(),\n",
    "    (('skills_j', 'skills_c'), ('fitness_skills',)): MatchFeatureSet(),\n",
    "    (('gender_j', 'gender_c'), ('fitness_gender',)): MatchBinary(),\n",
    "    (('agg_perceived_foreign_j', 'agg_perceived_foreign_c'), ('fitness_foreign',)): MatchBinary()\n",
    "}\n",
    "\n",
    "# Helper variable for the fitness features\n",
    "list_cols_fitness = ['fitness_experience', 'fitness_education', 'fitness_skills', 'fitness_gender', 'fitness_foreign']\n",
    "maps_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4d892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn transformation for numeric and categorical features\n",
    "\n",
    "numeric_features = list_cols_fitness\n",
    "categorical_features = ['gender_c', 'agg_perceived_foreign_c']\n",
    "# imputing and scaling numeric features\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), # Not needed for the used dataset.\n",
    "        (\"scaler\", StandardScaler()) # Not needed for the decision tree, let's keep it for the sake of generality.\n",
    "    ]\n",
    ")\n",
    "# imputing and encoding categorical features\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), # Not needed for the used dataset, again for the sake of generality.\n",
    "        (\"encoder\", OneHotEncoder()), # Convert to one-hot encoding\n",
    "    ]\n",
    ")\n",
    "# combining the two above\n",
    "column_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        # (\"cat\", categorical_transformer, categorical_features)\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# The pipeline is composed of two phases:\n",
    "# 1. Preprocessing with metadata (using findhr package)\n",
    "pipeline_derived = Pipeline(steps=[\n",
    "    (\"init\", AttachMetadata(md_all)),\n",
    "    ('mapping_1', DerivedColumn(maps_derived_1)),\n",
    "    ('mapping_2', DerivedColumn(maps_derived_2)),\n",
    "    (\"matching\", DerivedColumn(maps_matching)),\n",
    "    # (\"fitness\", GroundTruthLinearWeightedScorer(gt_weights_fair)),\n",
    "    (\"end\", DetachMetadata())\n",
    "])\n",
    "# 2. Standard scikit-learn preprocessing to prepare the data for the model covered by column preprocessor.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d75a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline definition for regression model on the target feature \"score\".\n",
    "# Note that LGBMRanker is not fully compatible with sklearn Pipeline\n",
    "# https://github.com/microsoft/LightGBM/issues/5041#issuecomment-1054827692\n",
    "\n",
    "pipeline_rank = Pipeline(\n",
    "    steps=[\n",
    "        # first phase: preprocessing with metadata\n",
    "        ('fitness_value', pipeline_derived),\n",
    "        # second phase: preprocessing without metadata (standard scikit-learn)\n",
    "        (\"column_preprocessor\", column_preprocessor),\n",
    "        # model inference\n",
    "        # (\"ranker\", LGBMRanker( # Define the ranking model\n",
    "        #     objective = \"lambdarank\",\n",
    "        #     class_weight = \"balanced\",\n",
    "        #     boosting_type = \"gbdt\",\n",
    "        #     importance_type = \"gain\",\n",
    "        #     learning_rate = 0.1,\n",
    "        #     n_estimators = 10,\n",
    "        #     force_row_wise = True,\n",
    "        #     verbose = -1              # no verbosity\n",
    "        # ))\n",
    "       ]\n",
    ")\n",
    "\n",
    "ranker = LGBMRanker( # Define the ranking model\n",
    "            objective = \"lambdarank\",\n",
    "            class_weight = \"balanced\",\n",
    "            boosting_type = \"gbdt\",\n",
    "            importance_type = \"gain\",\n",
    "            learning_rate = 0.1,\n",
    "            min_data_in_leaf = 10,\n",
    "            n_estimators = 10,\n",
    "            force_row_wise = True,\n",
    "            verbose = -1           # no verbosity during training, no warnings\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the top_K number for each job_id\n",
    "TOP_K = 10\n",
    "# Helper function to transform the rank into relevance, to train the LGBMRanker through the ndcg_score\n",
    "# LightGBM relevance is the higher the better\n",
    "def rank2relevance(df):\n",
    "    # Convert the ranking col to a relevance scale from 0 to TOP_K\n",
    "    return np.maximum(TOP_K + 1 - df['ranking'].values.ravel(), 0)\n",
    "\n",
    "# Note the first time we call the pipeline, it will fit the metadata and the transformations\n",
    "transformed_data = pipeline_rank.fit_transform(df_train.loc[:, pred_cols])\n",
    "transformed_val_data = pipeline_rank.transform(df_val.loc[:, pred_cols])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fitting ranker:\n",
    "fitting_params = dict(\n",
    "    X = transformed_data,\n",
    "    y =  rank2relevance(df_train),\n",
    "    group = df_train_counts,\n",
    "    eval_at = [TOP_K],\n",
    "    eval_set =[(transformed_val_data, rank2relevance(df_val))],\n",
    "    eval_group =[df_val_counts]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ccffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker.fit(**fitting_params)\n",
    "# pipeline_rank.fit(df_train.loc[:, pred_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction.\n",
    "transformed_test_data = pipeline_rank.transform(df_test.loc[:, pred_cols])\n",
    "lambda_pred = ranker.predict(transformed_test_data)\n",
    "lambda_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show an example of the relevance for the test dataset\n",
    "test_relevance = rank2relevance(df_test).reshape(1, -1)\n",
    "test_relevance.ravel()[np.where(test_relevance)[1]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model validation\n",
    "In this example, we do not have enough jobs to obtain proper train, validation and test data splits.\n",
    "Therefore, we validate the model by checking the relation between the \"lambdas\" predicted by the LGBMRanker model, and the score, separately for each job offer.\n",
    "To better understand the LambdaRank framework, we point to the relevant literature (e.g. [Burges, 2010, From RankNet to LambdaRank to LambdaMART: An Overview](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_all['lambda_pred'] = lambda_pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We now show that there is a clear trend of correlation between the score and the predicted lambda from the models for each job_id\n",
    "# Let's describe the prediction with a scatterplot\n",
    "\n",
    "# Set plotting variables\n",
    "cmap = plt.get_cmap('Set1')\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# export the unique ids for jobs\n",
    "unique_ids = df_all['id_j'].unique()\n",
    "\n",
    "# Produce a plot for each unique job_id\n",
    "for i, uid in enumerate(df_all['id_j'].unique()):\n",
    "    subset = df_all[df_all['id_j'] == uid]\n",
    "    color = cmap(i / len(unique_ids))\n",
    "\n",
    "    plt.scatter(subset['lambda_pred'], subset['score'],\n",
    "                color=color,\n",
    "                label=str(uid), edgecolor='black', alpha=0.7)\n",
    "\n",
    "    # If there is more than one data point, compute and plot the trendline.\n",
    "    if len(subset) > 1:\n",
    "        coeffs = np.polyfit(subset['lambda_pred'], subset['score'], 1)\n",
    "        p = np.poly1d(coeffs)\n",
    "        x_vals = np.linspace(subset['lambda_pred'].min(), subset['lambda_pred'].max(), 100)\n",
    "        y_vals = p(x_vals)\n",
    "\n",
    "        plt.plot(x_vals, y_vals, color=color, linestyle='--', linewidth=2)\n",
    "\n",
    "# Finalize plotting\n",
    "plt.xlabel('lambda_pred')\n",
    "plt.ylabel('score')\n",
    "plt.legend(title='id_j')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ndcg_score(rank2relevance(df_test).reshape(1, -1), ranker.predict(transformed_test_data).reshape(1, -1), k=TOP_K)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example Model Explanation with RankingSHAP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from findhr.xai.factual.ranking_shap import RankingShap\n",
    "from scipy.stats import kendalltau"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the background data\n",
    "# we use the training data of the candidates applying for the job with id_j = 3\n",
    "background_data = pipeline_rank.transform(df_train[df_train['id_j'] == 3][pred_cols])\n",
    "\n",
    "# Define the rank similarity coefficient for comparing the predicted ranking for explanations\n",
    "rank_similarity_coefficient = lambda x, y: kendalltau(x, y)[0]\n",
    "# rank_similarity_coefficient = lambda x, y: ndcg_score(x.reshape(1, -1), y.reshape(1, -1), k=TOP_K)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ranking_shap_explainer = RankingShap(\n",
    "    permutation_sampler=\"kernel\",\n",
    "    background_data=background_data,\n",
    "    original_model=ranker.predict,\n",
    "    name=\"rankingshap\",\n",
    "    rank_similarity_coefficient=rank_similarity_coefficient,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select the explicand data\n",
    "explicand_data = df_train[df_train['id_j'] == 3][pred_cols] #df_train[pred_cols].iloc[200:210, :]\n",
    "# Transform the explicand data\n",
    "transformed_explicand_data = pipeline_rank.transform(explicand_data)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the explanation\n",
    "out_exp = ranking_shap_explainer.get_query_explanation(transformed_explicand_data)\n",
    "\n",
    "# Get the feature importance for the explicand data after renaming the features as fitness features\n",
    "out_exp_renamed = {list_cols_fitness[k-1]: v for k, v in dict(out_exp).items()}\n",
    "out_exp_renamed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The most important features are skills and experience. The others have low importance. In particular, the small values of importance are due to approximation errors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(list(out_exp_renamed.keys()), list(out_exp_renamed.values()))\n",
    "plt.xlabel('Feature Importance')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
